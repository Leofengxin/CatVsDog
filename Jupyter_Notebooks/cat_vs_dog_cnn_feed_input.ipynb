{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to load data =  225.58342790603638\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from scipy.misc import imread, imresize, imsave\n",
    "import scipy.io\n",
    "from scipy.io import savemat, loadmat\n",
    "from os import listdir, walk\n",
    "from os.path import isfile, join\n",
    "#import PIL\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "NUM_CLASSES = 2\n",
    "IMG_HEIGHT = 64\n",
    "IMG_WIDTH = 64\n",
    "IMG_CHANNELS = 3\n",
    "IMG_PIXELS = IMG_HEIGHT * IMG_WIDTH * IMG_CHANNELS\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "trainPath = '../data/train/'\n",
    "testPath = '../data/test'\n",
    "\n",
    "imgClasses = ['dog', 'cat']\n",
    "numClasses = len(imgClasses)\n",
    "numTrain = len(next(walk(trainPath))[2])\n",
    "\n",
    "def readData(path):\n",
    "    fileNames = next(walk(trainPath))[2]\n",
    "    numTrain = len(fileNames)\n",
    "    images = np.zeros((numTrain, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS), dtype=np.float32)\n",
    "    labels = np.zeros((numTrain), dtype=np.int32)\n",
    "    for i, fileName in enumerate(fileNames):\n",
    "        img = imread(join(trainPath, fileName))\n",
    "        img = imresize(img, (IMG_HEIGHT, IMG_WIDTH))\n",
    "        images[i, :, :, :] = img\n",
    "\n",
    "        labels[i] = 0*(fileName[0:3] == imgClasses[0]) + 1*(fileName[0:3] == imgClasses[1])\n",
    "\n",
    "    images = (images) * (1. / 255) - 0.5\n",
    "    return images, labels\n",
    "\n",
    "train_images, train_labels = readData(trainPath)\n",
    "test_images, test_labels = readData(testPath)\n",
    "\n",
    "print('Time taken to load data = ', time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 1e-3\n",
    "REG_STRENGTH = 0.001\n",
    "DROP_PROB = 0.5\n",
    "DATA_DIR = 'data/'                     # Local CPU\n",
    "#DATA_DIR = '/data1/ankur/CatVsDog/'      # Berkeley GPU\n",
    "\n",
    "NUM_ITER = 10000\n",
    "\n",
    "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
    "    var = tf.Variable(tf.truncated_normal(shape=shape, stddev=stddev, name=name))\n",
    "    if wd is not None:\n",
    "        weight_decay = tf.mul(tf.nn.l2_loss(var), wd, name='reg_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)\n",
    "    return var\n",
    "\n",
    "def inference(images):\n",
    "    # conv 1\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        weights = _variable_with_weight_decay('weights', shape=[5, 5, 3, 32], stddev=1/np.sqrt(5*5*3), wd=0.00)\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[32]))\n",
    "        conv = tf.nn.conv2d(images, weights, [1, 1, 1, 1], padding='SAME')\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(bias, name=scope.name)\n",
    "\n",
    "    # conv 2\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        weights = _variable_with_weight_decay('weights', shape=[5, 5, 32, 64], stddev=1/np.sqrt(5*5*32), wd=0.00)\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[64]))\n",
    "        conv = tf.nn.conv2d(conv1, weights, [1, 1, 1, 1], padding='SAME')\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        conv2 = tf.nn.relu(bias, name=scope.name)\n",
    "\n",
    "    # pool 1\n",
    "    with tf.variable_scope('pool1') as scope:\n",
    "        pool1 = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "    # conv 3\n",
    "    with tf.variable_scope('conv3') as scope:\n",
    "        weights = _variable_with_weight_decay('weights', shape=[3, 3, 64, 64], stddev=1/np.sqrt(3*3*64), wd=0.00)\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[64]))\n",
    "        conv = tf.nn.conv2d(pool1, weights, [1, 1, 1, 1], padding='SAME')\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        conv3 = tf.nn.relu(bias, name=scope.name)\n",
    "\n",
    "    # conv 4\n",
    "    with tf.variable_scope('conv4') as scope:\n",
    "        weights = _variable_with_weight_decay('weights', shape=[3, 3, 64, 64], stddev=1/np.sqrt(3*3*64), wd=0.00)\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[64]))\n",
    "        conv = tf.nn.conv2d(conv3, weights, [1, 1, 1, 1], padding='SAME')\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        conv4 = tf.nn.relu(bias, name=scope.name)\n",
    "\n",
    "    # pool 2\n",
    "    with tf.variable_scope('pool2') as scope:\n",
    "        pool2 = tf.nn.max_pool(conv4, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "    # conv 5\n",
    "    with tf.variable_scope('conv5') as scope:\n",
    "        weights = _variable_with_weight_decay('weights', shape=[3, 3, 64, 64], stddev=1/np.sqrt(3*3*64), wd=0.00)\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[64]))\n",
    "        conv = tf.nn.conv2d(pool2, weights, [1, 1, 1, 1], padding='SAME')\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        conv5 = tf.nn.relu(bias, name=scope.name)\n",
    "\n",
    "    # conv 6\n",
    "    with tf.variable_scope('conv6') as scope:\n",
    "        weights = _variable_with_weight_decay('weights', shape=[3, 3, 64, 64], stddev=1/np.sqrt(3*3*64), wd=0.00)\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[64]))\n",
    "        conv = tf.nn.conv2d(conv5, weights, [1, 1, 1, 1], padding='SAME')\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        conv6 = tf.nn.relu(bias, name=scope.name)\n",
    "\n",
    "    # pool 3\n",
    "    with tf.variable_scope('pool3') as scope:\n",
    "        pool3 = tf.nn.max_pool(conv6, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "\n",
    "    # fully connected 1\n",
    "    with tf.variable_scope('fc1') as scope:\n",
    "        batch_size = images.get_shape()[0].value\n",
    "        pool3_flat = tf.reshape(pool3, [batch_size, -1])\n",
    "        dim = pool3_flat.get_shape()[1].value\n",
    "        weights = _variable_with_weight_decay('weights', shape=[dim, 384], stddev=1/np.sqrt(dim), wd=REG_STRENGTH)\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[384]))\n",
    "        fc1 = tf.nn.relu(tf.matmul(pool3_flat, weights) + biases, name=scope.name)\n",
    "\n",
    "    # fully connected 2\n",
    "    with tf.variable_scope('fc2') as scope:\n",
    "        weights = _variable_with_weight_decay('weights', shape=[384, 192], stddev=1/np.sqrt(384), wd=REG_STRENGTH)\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[192]))\n",
    "        fc2 = tf.nn.relu(tf.matmul(fc1, weights) + biases, name=scope.name)\n",
    "\n",
    "    # dropout\n",
    "        fc2_drop = tf.nn.dropout(fc2, DROP_PROB)\n",
    "\n",
    "    # Softmax\n",
    "    with tf.variable_scope('softmax_linear') as scope:\n",
    "        weights = _variable_with_weight_decay('weights', shape=[192, NUM_CLASSES], stddev=1/np.sqrt(192), wd=0.000)\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[NUM_CLASSES]))\n",
    "        # softmax_linear = tf.nn.softmax(tf.matmul(fc2_drop, weights) + biases, name=scope.name)#<--BLUNDER!\n",
    "        logits = tf.add(tf.matmul(fc2_drop, weights), biases, name=scope.name)\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "def loss(logits, labels):\n",
    "    labels = tf.cast(labels, tf.int64)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name='xentropy')\n",
    "    data_loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "    tf.add_to_collection('losses', data_loss)\n",
    "    total_loss = tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def training(total_loss, learning_rate):\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    train_op = optimizer.minimize(total_loss, global_step=global_step)\n",
    "\n",
    "    return train_op\n",
    "\n",
    "\n",
    "def evaluation(logits, true_labels):\n",
    "    correct_pred = tf.nn.in_top_k(logits, true_labels, 1)\n",
    "    return tf.reduce_mean(tf.cast(correct_pred, tf.float32))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    with tf.Graph().as_default():\n",
    "        X = tf.placeholder(tf.float32, [BATCH_SIZE, IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS])\n",
    "        y = tf.placeholder(tf.int32, [BATCH_SIZE])\n",
    "\n",
    "        logits = inference(X)\n",
    "\n",
    "        total_loss = loss(logits, y)\n",
    "\n",
    "        train_op = training(total_loss, learning_rate=LEARNING_RATE)\n",
    "\n",
    "        accuracy = evaluation(logits, y)\n",
    "\n",
    "        # Don't specify number of epochs in validation set, otherwise that limits the training duration as the\n",
    "        # validation set is 10 times smaller than the training set\n",
    "        #val_logits = model_cnn.inference(val_images)\n",
    "        #val_accuracy = model_cnn.evaluation(val_logits, val_labels)\n",
    "\n",
    "        init_op = tf.initialize_all_variables()\n",
    "\n",
    "        sess = tf.Session()\n",
    "\n",
    "        sess.run(init_op)\n",
    "\n",
    "        for i in range(NUM_ITER):\n",
    "            start_time = time.time()\n",
    "            sampleIndices = np.random.choice(np.arange(numTrain), BATCH_SIZE)\n",
    "            batch_xs = train_images[sampleIndices]   #\n",
    "            batch_ys = train_labels[sampleIndices]\n",
    "            sess.run(train_op, feed_dict={X: batch_xs, y: batch_ys})\n",
    "            duration = time.time() - start_time\n",
    "            if i%10 == 0:\n",
    "                #print(sess.run(logits, feed_dict={X:batch_xs, y: batch_ys}))\n",
    "                print(\"Iteration = \", i, \"Loss = \", sess.run(total_loss, feed_dict={X:batch_xs, y: batch_ys}),\n",
    "                      \"Train Accuracy = \", sess.run(accuracy, feed_dict={X:batch_xs, y: batch_ys}), \n",
    "                      \"Test Accuracy = \", sess.run(accuracy, feed_dict={X:test_images[np.arange(BATCH_SIZE)], y: test_labels[np.arange(BATCH_SIZE)]}), \n",
    "                      \"Duration = %.1f sec\" % duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Iteration =  0 Loss =  0.897286 Train Accuracy =  56.0 Test Accuracy =  49.0 Duration = 3.2 sec\n",
      "Iteration =  10 Loss =  0.804559 Train Accuracy =  50.0 Test Accuracy =  49.0 Duration = 5.7 sec\n",
      "Iteration =  20 Loss =  0.765104 Train Accuracy =  46.0 Test Accuracy =  49.0 Duration = 7.4 sec\n",
      "Iteration =  30 Loss =  0.733826 Train Accuracy =  53.0 Test Accuracy =  58.0 Duration = 9.4 sec\n",
      "Iteration =  40 Loss =  0.702011 Train Accuracy =  56.0 Test Accuracy =  50.0 Duration = 5.8 sec\n",
      "Iteration =  50 Loss =  0.712868 Train Accuracy =  50.0 Test Accuracy =  48.0 Duration = 6.3 sec\n",
      "Iteration =  60 Loss =  0.712052 Train Accuracy =  40.0 Test Accuracy =  67.0 Duration = 5.8 sec\n",
      "Iteration =  70 Loss =  0.706997 Train Accuracy =  48.0 Test Accuracy =  46.0 Duration = 5.7 sec\n",
      "Iteration =  80 Loss =  0.69991 Train Accuracy =  59.0 Test Accuracy =  51.0 Duration = 7.5 sec\n",
      "Iteration =  90 Loss =  0.674792 Train Accuracy =  58.0 Test Accuracy =  59.0 Duration = 6.0 sec\n",
      "Iteration =  100 Loss =  0.6989 Train Accuracy =  58.0 Test Accuracy =  64.0 Duration = 6.4 sec\n",
      "Iteration =  110 Loss =  0.692839 Train Accuracy =  59.0 Test Accuracy =  61.0 Duration = 3.9 sec\n",
      "Iteration =  120 Loss =  0.652578 Train Accuracy =  74.0 Test Accuracy =  67.0 Duration = 4.6 sec\n",
      "Iteration =  130 Loss =  0.644196 Train Accuracy =  72.0 Test Accuracy =  65.0 Duration = 4.3 sec\n",
      "Iteration =  140 Loss =  0.658172 Train Accuracy =  62.0 Test Accuracy =  66.0 Duration = 3.9 sec\n",
      "Iteration =  150 Loss =  0.681862 Train Accuracy =  61.0 Test Accuracy =  71.0 Duration = 3.7 sec\n",
      "Iteration =  160 Loss =  0.665223 Train Accuracy =  62.0 Test Accuracy =  71.0 Duration = 3.7 sec\n",
      "Iteration =  170 Loss =  0.601573 Train Accuracy =  68.0 Test Accuracy =  59.0 Duration = 3.9 sec\n",
      "Iteration =  180 Loss =  0.652719 Train Accuracy =  61.0 Test Accuracy =  72.0 Duration = 7.1 sec\n",
      "Iteration =  190 Loss =  0.567847 Train Accuracy =  74.0 Test Accuracy =  75.0 Duration = 5.9 sec\n",
      "Iteration =  200 Loss =  0.529146 Train Accuracy =  74.0 Test Accuracy =  73.0 Duration = 5.5 sec\n",
      "Iteration =  210 Loss =  0.549818 Train Accuracy =  73.0 Test Accuracy =  71.0 Duration = 3.8 sec\n",
      "Iteration =  220 Loss =  0.611544 Train Accuracy =  66.0 Test Accuracy =  70.0 Duration = 6.7 sec\n",
      "Iteration =  230 Loss =  0.629666 Train Accuracy =  66.0 Test Accuracy =  77.0 Duration = 6.6 sec\n",
      "Iteration =  240 Loss =  0.581433 Train Accuracy =  73.0 Test Accuracy =  74.0 Duration = 4.3 sec\n",
      "Iteration =  250 Loss =  0.581033 Train Accuracy =  73.0 Test Accuracy =  76.0 Duration = 4.2 sec\n",
      "Iteration =  260 Loss =  0.599253 Train Accuracy =  73.0 Test Accuracy =  73.0 Duration = 5.9 sec\n",
      "Iteration =  270 Loss =  0.58701 Train Accuracy =  70.0 Test Accuracy =  76.0 Duration = 4.3 sec\n",
      "Iteration =  280 Loss =  0.553387 Train Accuracy =  73.0 Test Accuracy =  74.0 Duration = 4.1 sec\n",
      "Iteration =  290 Loss =  0.605001 Train Accuracy =  69.0 Test Accuracy =  71.0 Duration = 6.4 sec\n",
      "Iteration =  300 Loss =  0.676534 Train Accuracy =  65.0 Test Accuracy =  72.0 Duration = 5.8 sec\n",
      "Iteration =  310 Loss =  0.598188 Train Accuracy =  64.0 Test Accuracy =  69.0 Duration = 4.6 sec\n",
      "Iteration =  320 Loss =  0.475671 Train Accuracy =  83.0 Test Accuracy =  77.0 Duration = 4.8 sec\n",
      "Iteration =  330 Loss =  0.466821 Train Accuracy =  81.0 Test Accuracy =  72.0 Duration = 4.3 sec\n",
      "Iteration =  340 Loss =  0.55727 Train Accuracy =  72.0 Test Accuracy =  64.0 Duration = 4.4 sec\n",
      "Iteration =  350 Loss =  0.550564 Train Accuracy =  78.0 Test Accuracy =  76.0 Duration = 4.2 sec\n",
      "Iteration =  360 Loss =  0.555048 Train Accuracy =  73.0 Test Accuracy =  81.0 Duration = 3.9 sec\n",
      "Iteration =  370 Loss =  0.499557 Train Accuracy =  72.0 Test Accuracy =  70.0 Duration = 3.7 sec\n",
      "Iteration =  380 Loss =  0.553618 Train Accuracy =  75.0 Test Accuracy =  75.0 Duration = 3.9 sec\n",
      "Iteration =  390 Loss =  0.413951 Train Accuracy =  85.0 Test Accuracy =  77.0 Duration = 6.4 sec\n",
      "Iteration =  400 Loss =  0.544774 Train Accuracy =  72.0 Test Accuracy =  70.0 Duration = 6.9 sec\n",
      "Iteration =  410 Loss =  0.481867 Train Accuracy =  75.0 Test Accuracy =  79.0 Duration = 3.8 sec\n",
      "Iteration =  420 Loss =  0.527968 Train Accuracy =  73.0 Test Accuracy =  77.0 Duration = 3.7 sec\n",
      "Iteration =  430 Loss =  0.412916 Train Accuracy =  84.0 Test Accuracy =  76.0 Duration = 3.8 sec\n",
      "Iteration =  440 Loss =  0.537862 Train Accuracy =  76.0 Test Accuracy =  75.0 Duration = 3.8 sec\n",
      "Iteration =  450 Loss =  0.516265 Train Accuracy =  78.0 Test Accuracy =  75.0 Duration = 8.1 sec\n",
      "Iteration =  460 Loss =  0.50987 Train Accuracy =  76.0 Test Accuracy =  79.0 Duration = 9.6 sec\n",
      "Iteration =  470 Loss =  0.437321 Train Accuracy =  83.0 Test Accuracy =  77.0 Duration = 4.9 sec\n",
      "Iteration =  480 Loss =  0.422059 Train Accuracy =  80.0 Test Accuracy =  78.0 Duration = 4.9 sec\n",
      "Iteration =  490 Loss =  0.458247 Train Accuracy =  80.0 Test Accuracy =  74.0 Duration = 4.4 sec\n",
      "Iteration =  500 Loss =  0.543543 Train Accuracy =  72.0 Test Accuracy =  73.0 Duration = 4.6 sec\n",
      "Iteration =  510 Loss =  0.551881 Train Accuracy =  74.0 Test Accuracy =  72.0 Duration = 6.7 sec\n",
      "Iteration =  520 Loss =  0.426967 Train Accuracy =  79.0 Test Accuracy =  77.0 Duration = 4.0 sec\n",
      "Iteration =  530 Loss =  0.438176 Train Accuracy =  80.0 Test Accuracy =  75.0 Duration = 4.2 sec\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-1c56b07f89c3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#inference(1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mrun_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-13-43705943104f>\u001b[0m in \u001b[0;36mrun_training\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mbatch_xs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_images\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msampleIndices\u001b[0m\u001b[1;33m]\u001b[0m   \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mbatch_ys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0msampleIndices\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m             \u001b[0mduration\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/robo/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 372\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    373\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/robo/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 636\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    637\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m       \u001b[1;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/robo/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    706\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 708\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    709\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/home/robo/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    713\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 715\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    716\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/robo/.local/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    695\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m    696\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 697\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#inference(1)\n",
    "print()\n",
    "run_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
