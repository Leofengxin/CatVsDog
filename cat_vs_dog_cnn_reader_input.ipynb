{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import read_data\n",
    "import read_data_val\n",
    "import model_cnn\n",
    "import tensorflow as tf\n",
    "\n",
    "from os.path import join\n",
    "import tensorflow as tf\n",
    "\n",
    "# Constants used for dealing with the files, matches convert_to_records.\n",
    "TRAIN_FILE = 'train.tfrecords'\n",
    "VALIDATION_FILE = 'validation.tfrecords'\n",
    "TEST_FILE = 'test.tfrecords'\n",
    "#DATA_DIR = 'data/'                     # Local CPU\n",
    "DATA_DIR = '/data1/ankur/CatVsDog/'      # Berkeley GPU\n",
    "NUM_CLASSES = 2\n",
    "IMG_HEIGHT = 128\n",
    "IMG_WIDTH = 128\n",
    "IMG_CHANNELS = 3\n",
    "IMG_PIXELS = IMG_HEIGHT * IMG_WIDTH * IMG_CHANNELS\n",
    "\n",
    "\n",
    "def dense_to_one_hot(labels_dense, num_classes):\n",
    "    \"\"\"Convert class labels from scalars to one-hot vectors.\"\"\"\n",
    "    num_labels = labels_dense.shape[0]\n",
    "    index_offset = np.arange(num_labels) * num_classes\n",
    "    labels_one_hot = np.zeros((num_labels, num_classes))\n",
    "    labels_one_hot.flat[index_offset + labels_dense.ravel()] = 1\n",
    "    return labels_one_hot\n",
    "\n",
    "\n",
    "def read_and_decode(filename_queue):\n",
    "    reader = tf.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    features = tf.parse_single_example(\n",
    "        serialized_example,\n",
    "        # Defaults are not specified since both keys are required.\n",
    "        features={\n",
    "            'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "            'label': tf.FixedLenFeature([], tf.int64),\n",
    "            'height': tf.FixedLenFeature([], tf.int64),\n",
    "            'width': tf.FixedLenFeature([], tf.int64),\n",
    "            'depth': tf.FixedLenFeature([], tf.int64)\n",
    "        })\n",
    "\n",
    "    image = tf.decode_raw(features['image_raw'], tf.uint8)\n",
    "    img_height = tf.cast(features['height'], tf.int32)\n",
    "    img_width = tf.cast(features['width'], tf.int32)\n",
    "    img_depth = tf.cast(features['depth'], tf.int32)\n",
    "    # Convert label from a scalar uint8 tensor to an int32 scalar.\n",
    "    label = tf.cast(features['label'], tf.int32)\n",
    "\n",
    "    image.set_shape([IMG_PIXELS])\n",
    "    image = tf.reshape(image, [IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS])\n",
    "\n",
    "    # Convert from [0, 255] -> [-0.5, 0.5] floats.\n",
    "    image = tf.cast(image, tf.float32) * (1. / 255) - 0.5\n",
    "\n",
    "    return image, label\n",
    "\n",
    "\n",
    "def inputs(data_set, batch_size, num_epochs):\n",
    "    \"\"\"Reads input data num_epochs times.\n",
    "    Args:\n",
    "    train: Selects between the train , validation and test data.\n",
    "    batch_size: Number of examples per returned batch.\n",
    "    num_epochs: Number of times to read the input data, or 0/None to\n",
    "       train forever.\n",
    "    Returns:\n",
    "    A tuple (images, labels), where:\n",
    "    * images is a float tensor with shape [batch_size, mnist.IMAGE_PIXELS]\n",
    "      in the range [-0.5, 0.5].\n",
    "    * labels is an int32 tensor with shape [batch_size] with the true label,\n",
    "      a number in the range [0, mnist.NUM_CLASSES).\n",
    "    Note that an tf.train.QueueRunner is added to the graph, which\n",
    "    must be run using e.g. tf.train.start_queue_runners().\n",
    "    \"\"\"\n",
    "    if not num_epochs:\n",
    "        num_epochs = None\n",
    "    if data_set == 'train':\n",
    "        file = TRAIN_FILE\n",
    "    elif data_set == 'validation':\n",
    "        file = VALIDATION_FILE\n",
    "    elif data_set == 'test':\n",
    "        file = TEST_FILE\n",
    "    else:\n",
    "        raise ValueError('data_set should be one of \\'train\\', \\'validation\\' or \\'test\\'')\n",
    "    filename = join(DATA_DIR, file)\n",
    "\n",
    "    with tf.name_scope('input'):\n",
    "        filename_queue = tf.train.string_input_producer(\n",
    "            [filename], num_epochs=num_epochs)\n",
    "\n",
    "    # Even when reading in multiple threads, share the filename\n",
    "    # queue.\n",
    "    image, label = read_and_decode(filename_queue)\n",
    "\n",
    "    # Shuffle the examples and collect them into batch_size batches.\n",
    "    # (Internally uses a RandomShuffleQueue.)\n",
    "    # We run this in two threads to avoid being a bottleneck.\n",
    "    images, sparse_labels = tf.train.shuffle_batch(\n",
    "        [image, label], batch_size=batch_size, num_threads=2,\n",
    "        capacity=1000 + 3 * batch_size,\n",
    "        # Ensures a minimum amount of shuffling of examples.\n",
    "        min_after_dequeue=1000)\n",
    "\n",
    "    return images, sparse_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "NUM_EPOCHS = 100\n",
    "LEARNING_RATE = 1e-3\n",
    "DROP_PROB = 0.5\n",
    "DATA_DIR = 'data/'                     # Local CPU\n",
    "#DATA_DIR = '/data1/ankur/CatVsDog/'      # Berkeley GPU\n",
    "\n",
    "NUM_ITER = 10000\n",
    "\n",
    "def _variable_with_weight_decay(name, shape, stddev, wd):\n",
    "    var = tf.Variable(tf.truncated_normal(shape=shape, stddev=stddev, name=name))\n",
    "    if wd is not None:\n",
    "        weight_decay = tf.mul(tf.nn.l2_loss(var), wd, name='reg_loss')\n",
    "        tf.add_to_collection('losses', weight_decay)\n",
    "    return var\n",
    "\n",
    "\n",
    "def inference(images):\n",
    "    # conv 1\n",
    "    with tf.variable_scope('conv1') as scope:\n",
    "        weights = _variable_with_weight_decay('weights', shape=[5, 5, 3, 32], stddev=1/np.sqrt(5*5*3), wd=0.00)\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[32]))\n",
    "        conv = tf.nn.conv2d(images, weights, [1, 1, 1, 1], padding='SAME')\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        conv1 = tf.nn.relu(bias, name=scope.name)\n",
    "\n",
    "    # conv 2\n",
    "    with tf.variable_scope('conv2') as scope:\n",
    "        weights = _variable_with_weight_decay('weights', shape=[5, 5, 32, 64], stddev=1/np.sqrt(5*5*32), wd=0.00)\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[64]))\n",
    "        conv = tf.nn.conv2d(conv1, weights, [1, 1, 1, 1], padding='SAME')\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        conv2 = tf.nn.relu(bias, name=scope.name)\n",
    "\n",
    "    # pool 1\n",
    "    with tf.variable_scope('pool1') as scope:\n",
    "        pool1 = tf.nn.max_pool(conv2, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "    # conv 3\n",
    "    with tf.variable_scope('conv3') as scope:\n",
    "        weights = _variable_with_weight_decay('weights', shape=[3, 3, 64, 64], stddev=1/np.sqrt(3*3*64), wd=0.00)\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[64]))\n",
    "        conv = tf.nn.conv2d(pool1, weights, [1, 1, 1, 1], padding='SAME')\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        conv3 = tf.nn.relu(bias, name=scope.name)\n",
    "\n",
    "    # conv 4\n",
    "    with tf.variable_scope('conv4') as scope:\n",
    "        weights = _variable_with_weight_decay('weights', shape=[3, 3, 64, 64], stddev=1/np.sqrt(3*3*64), wd=0.00)\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[64]))\n",
    "        conv = tf.nn.conv2d(conv3, weights, [1, 1, 1, 1], padding='SAME')\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        conv4 = tf.nn.relu(bias, name=scope.name)\n",
    "\n",
    "    # pool 2\n",
    "    with tf.variable_scope('pool2') as scope:\n",
    "        pool2 = tf.nn.max_pool(conv4, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "    # conv 5\n",
    "    with tf.variable_scope('conv5') as scope:\n",
    "        weights = _variable_with_weight_decay('weights', shape=[3, 3, 64, 64], stddev=1/np.sqrt(3*3*64), wd=0.00)\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[64]))\n",
    "        conv = tf.nn.conv2d(pool2, weights, [1, 1, 1, 1], padding='SAME')\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        conv5 = tf.nn.relu(bias, name=scope.name)\n",
    "\n",
    "    # conv 6\n",
    "    with tf.variable_scope('conv6') as scope:\n",
    "        weights = _variable_with_weight_decay('weights', shape=[3, 3, 64, 64], stddev=1/np.sqrt(3*3*64), wd=0.00)\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[64]))\n",
    "        conv = tf.nn.conv2d(conv5, weights, [1, 1, 1, 1], padding='SAME')\n",
    "        bias = tf.nn.bias_add(conv, biases)\n",
    "        conv6 = tf.nn.relu(bias, name=scope.name)\n",
    "\n",
    "    # pool 3\n",
    "    with tf.variable_scope('pool3') as scope:\n",
    "        pool3 = tf.nn.max_pool(conv6, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "\n",
    "\n",
    "    # fully connected 1\n",
    "    with tf.variable_scope('fc1') as scope:\n",
    "        batch_size = images.get_shape()[0].value\n",
    "        pool3_flat = tf.reshape(pool3, [batch_size, -1])\n",
    "        dim = pool3_flat.get_shape()[1].value\n",
    "        weights = _variable_with_weight_decay('weights', shape=[dim, 384], stddev=1/np.sqrt(dim), wd=REG_STRENGTH)\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[384]))\n",
    "        fc1 = tf.nn.relu(tf.matmul(pool3_flat, weights) + biases, name=scope.name)\n",
    "\n",
    "    # fully connected 2\n",
    "    with tf.variable_scope('fc2') as scope:\n",
    "        weights = _variable_with_weight_decay('weights', shape=[384, 192], stddev=1/np.sqrt(384), wd=REG_STRENGTH)\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[192]))\n",
    "        fc2 = tf.nn.relu(tf.matmul(fc1, weights) + biases, name=scope.name)\n",
    "\n",
    "    # dropout\n",
    "        fc2_drop = tf.nn.dropout(fc2, DROP_PROB)\n",
    "\n",
    "    # Softmax\n",
    "    with tf.variable_scope('softmax_linear') as scope:\n",
    "        weights = _variable_with_weight_decay('weights', shape=[192, NUM_CLASSES], stddev=1/np.sqrt(192), wd=0.000)\n",
    "        biases = tf.Variable(tf.constant(0.0, shape=[NUM_CLASSES]))\n",
    "        # softmax_linear = tf.nn.softmax(tf.matmul(fc2_drop, weights) + biases, name=scope.name)#<--BLUNDER!\n",
    "        logits = tf.add(tf.matmul(fc2_drop, weights), biases, name=scope.name)\n",
    "\n",
    "    return logits\n",
    "\n",
    "\n",
    "def lossfn(logits, labels):\n",
    "    labels = tf.cast(labels, tf.int64)\n",
    "    cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits, labels, name='xentropy')\n",
    "    data_loss = tf.reduce_mean(cross_entropy, name='xentropy_mean')\n",
    "    tf.add_to_collection('losses', data_loss)\n",
    "    total_loss = tf.add_n(tf.get_collection('losses'), name='total_loss')\n",
    "    return total_loss\n",
    "\n",
    "\n",
    "def training(total_loss, learning_rate):\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "    global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "\n",
    "    train_op = optimizer.minimize(total_loss, global_step=global_step)\n",
    "\n",
    "    return train_op\n",
    "\n",
    "\n",
    "def evaluation(logits, true_labels):\n",
    "    correct_pred = tf.nn.in_top_k(logits, true_labels, 1)\n",
    "    return tf.reduce_sum(tf.cast(correct_pred, tf.int32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    with tf.Graph().as_default():\n",
    "        images, labels = inputs(data_set='train', batch_size=BATCH_SIZE, num_epochs=NUM_EPOCHS)\n",
    "\n",
    "        logits = inference(images)\n",
    "\n",
    "        loss = lossfn(logits, labels)\n",
    "\n",
    "        train_op = training(loss, learning_rate=LEARNING_RATE)\n",
    "\n",
    "        train_accuracy = evaluation(logits, labels)\n",
    "\n",
    "        # Don't specify number of epochs in validation set, otherwise that limits the training duration as the\n",
    "        # validation set is 10 times smaller than the training set\n",
    "        val_images, val_labels = inputs(data_set='validation', batch_size=BATCH_SIZE, num_epochs=None)\n",
    "        val_logits = inference(val_images)\n",
    "        val_accuracy = evaluation(val_logits, val_labels)\n",
    "\n",
    "        init_op = tf.initialize_all_variables()\n",
    "\n",
    "        sess = tf.Session()\n",
    "\n",
    "        sess.run(init_op)\n",
    "\n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "        try:\n",
    "            step = 0\n",
    "            while not coord.should_stop():\n",
    "                start_time = time.time()\n",
    "\n",
    "                _, loss_value, train_acc_val, valid_acc_val = sess.run([train_op, loss, train_accuracy, val_accuracy])\n",
    "\n",
    "                duration = time.time() - start_time\n",
    "                assert not np.isnan(loss_value), 'Model diverged with loss = NaN'\n",
    "\n",
    "                if step % 1 == 0:\n",
    "                    print('Step %d : loss = %.5f , training accuracy = %.1f, validation accuracy = %.1f (%.3f sec)'\n",
    "                          % (step, loss_value, train_acc_val, valid_acc_val, duration))\n",
    "                step += 1\n",
    "        except tf.errors.OutOfRangeError:\n",
    "            print('Done training for %d epochs, %d steps' % (NUM_EPOCHS, step))\n",
    "        finally:\n",
    "            coord.request_stop()\n",
    "\n",
    "        coord.join(threads)\n",
    "        sess.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0 : loss = 1.93766 , training accuracy = 100.0, validation accuracy = 100.0 (44.571 sec)\n",
      "Step 1 : loss = 1.72173 , training accuracy = 48.0, validation accuracy = 100.0 (39.994 sec)\n"
     ]
    }
   ],
   "source": [
    "#inference(1)\n",
    "run_training()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
